{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Models with latent variables\n",
    "![img](https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/HMMGraph.svg/2000px-HMMGraph.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e33df193-df05-4686-9b96-1e44b72ff893"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "![img](https://stathwang.github.io/images/hmm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c269adce-a228-4d80-bb8c-d082aa4cbf39"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov model\n",
    "Given set of states $S = {s_1, ..., s_m}$ we observe series over time $z \\in S^T$\n",
    "\n",
    "Assumptions about markov model:\n",
    "* Limited horizon  \n",
    "$P(z_t | z_{t-1}, z_{t-2}, ..., z_{t-n}) = P(z_{t} | z_{t-1})$\n",
    "\n",
    "* Stationary process  \n",
    "$P(z_{t} | z_{t-1}) = P(z_2 | z_1)$ for $t \\in 2..T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "44ba428c-f4e2-4f39-b162-83e4d5cb41fa"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "State transition matrix $A \\in R^{(|S|+1) x (|S|+1)}$, where  \n",
    "$A_{ij}$ is probabilty of transition from state i to state j\n",
    "\n",
    "We compute probability of the particular sequence z by chain rule:  \n",
    "$P(z_t, z_{t-1}, z_{t-2}, ..., z_1; A) = P(z_t | z_{t-1}, z_{t-2}, ..., z_0; A) =  \n",
    "= P(z_{t} | z_{t-1}; A) P(z_{t-1} | z_{t-2}; A) P(z_2 | z_1; A) P(z_1 | z_0; A) = \\\\ = \\prod_{t=1}^T P(z_{t} | z_{t-1}; A) = \\prod_{t=1}^T A_{z_{t-1}, z_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "45da01de-772c-4f72-80dd-04fb6cc3947c"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maximum likelihood parameter assignment\n",
    "\n",
    "$l(A) = log P(z; A) = log \\prod_{t=1}^T A_{z_{t-1}, z_t} = \\sum_{t=1}^T log A_{z_{t-1}, z_t} = \\sum_{t=1}^T \\sum_{i=1}^{|S|} \\sum_{j=1}^{|S|} [z_{t-1} = s_i \\wedge z_t = s_j]log A_{ij}$\n",
    "\n",
    "We want:  \n",
    "$l(A) -> \\underset {A} {\\max}$  \n",
    "$\\sum_{j=1}^{|S|} A_{ij} = 1 $  \n",
    "$A_{ij} \\geq 0$  \n",
    "\n",
    "With lagrange multipliers we'll get following estimate:  \n",
    "$\\hat A_{ij} = \\frac {\\sum_{t=1}^T  [z_{t-1} = s_i \\wedge z_t = s_j]} {\\sum_{t=1}^T  [z_{t-1} = s_i}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "07099aa9-36f0-49b3-b5a8-bd1e3b2adfc0"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hidden Markov Model\n",
    "\n",
    "$P(x; A, B) = \\sum_z P(x, z; A, B) = \\sum_z P(x|z; A, B) P(z; A, B) = \\sum_z ( \\prod_{t=1}^T P(x_t|z_t; B) ) ( \\prod_{t=1}^T P(z_t|z_{t-1}; A) ) = \\sum_z ( \\prod_{t=1}^T B_{z_t, x_t} ) ( \\prod_{t=1}^T A_{z_{t-1}, z_t} )$  \n",
    "Evaluating the prob directly costs $O(|S|^T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "31a1293e-8865-425b-9a3f-9ebc92493c34"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fundamental questions for HMM:\n",
    "* What is the probability of the observed sequence?  \n",
    "Given HMM (A, B) and observations x, caclulate probability that HMM generated x.\n",
    "* What is the most likely series of states to generate the observations?  \n",
    "Given HMM (A, B) and observations x, caclulate the most likely sequence of hidden states, that produced observations x.  \n",
    "* How can we learn A and B?  \n",
    "Given some training observations x and general structure of HMM (number of hidden and visible states), determine (A, B) that best fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "239c31c1-a42c-4e6e-93e0-ccd7ce66a243"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward algorithm \n",
    "for computing for computing probability of observed sequence:\n",
    "\n",
    "Define $\\alpha_i(t) = P(x_1, x_2, ..., x_t, z_t=s_i; A, B)$ - total probability of all observations up through time t, and being at state s_i at time t.\n",
    "Then,  \n",
    "$P(x; A, B) = P(x_1, ..., x_T; A, B) = \\sum_{i=1}^{|S|} P(x_1, ..., x_T, z_T = s_i; A, B)) = \\sum_{i=1}^{|S|} \\alpha_i(T)$\n",
    "\n",
    "We can compute with for $O(|S| T)$ by dynamic programming:  \n",
    "$\\alpha_i (0) = A_{0, i} $  \n",
    "$\\alpha_j (t) = \\sum_{i=1}^{|S|} \\alpha_i (t-1) A_{ij} B_{j, x_t} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0cbd0837-8f41-4679-ac65-aaff3086e7c2"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward algorithm\n",
    "\n",
    "Define $\\beta_i(t) = P(x_{t+1}, ..., x_T, z_t=s_i; A, B)$ - probability of observing the rest of the sequence after time step t being at state s_i.  \n",
    "$\\beta_i(T) = A_{i, 0}$   \n",
    "$\\beta_i(t) = \\sum_j A_{ij} B_{j, x_{t+1}} \\beta_j(t+1) $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "07ffd16b-95f4-4ea7-af7a-fe7b3f939b04"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Viterbi algorithm  \n",
    "for maximum likelihood state assignment. \n",
    "\n",
    "Given series of outputs $x \\in V^T$:  \n",
    "\n",
    "$arg max_z P(z|x; A, B) = arg max_z \\frac {P(x, z; A, B)} {\\sum_z P(x, z; A, B)} = arg max_z P(x, z; A, B)$\n",
    "\n",
    "Naive approach in $O(|S|^T)$.  \n",
    "\n",
    "Let $\\pi[j, s]$ - max probability for any state sequence ending in state s at position j.\n",
    "\n",
    "$\\pi[1, s] = A_{0, s} B_{s, x_1}$\n",
    "\n",
    "$\\pi[j, s] = max_{i \\in {1 .. k}} \\pi[j-1, i] A_{i, s} B_{s, x_j}$\n",
    "$bp[j, s] = arg max_{i \\in {1 .. k}} \\pi[j-1, i] A_{i, s} B_{s, x_j}$\n",
    "\n",
    "Recover all sequence:  \n",
    "$s_T = argmax_s \\pi[T, s]$  \n",
    "$s_{j-1} = bp[j, s_j]$\n",
    "\n",
    "Complexity $O(T |S|^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e1d88af6-2bbe-4bb5-8cc4-78795b45987f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### EM for HMM\n",
    "\n",
    "Repeat until convergence:\n",
    "\n",
    "E-step:  \n",
    "\n",
    "$Q(z) = p(z|x; A, B)$\n",
    "\n",
    "\n",
    "M-step:\n",
    "\n",
    "$A, B = arg max_{A, B} \\sum_z Q(z) log \\frac {P(x, z; A, B)} {Q(z)}$  \n",
    "s.t. $\\sum_{j=1}^{|S|} A_{ij} = 1$, $A_ij \\geq 0$  \n",
    "$\\sum_{k=1}^{|V|} B_{ik} = 1$, $B_ik \\geq 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9a842598-acd3-4eea-8b26-4dd8c3417dd8"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward-backward algorithm (Baum Welch)\n",
    "\n",
    "Init A, B as random probability matrices  \n",
    "$A_{i0} = 0$, $B_{0k} = 0$\n",
    "\n",
    "Repeat until convergence:  \n",
    "    \n",
    "E-step: \n",
    "    \n",
    "compute $\\alpha_i$ and $\\beta_i$ for i =1..S\n",
    "\n",
    "Set $\\gamma_t(i,j) = \\alpha_i(t) A_{ij} B_{j, x_t} \\beta_j(t+1)$ - proportional to the probability of transitioning between i and j states at time t.  \n",
    "\n",
    "M-step: \n",
    "\n",
    "Reestimate:\n",
    "\n",
    "$A_{ij} = \\frac { \\sum_{t=1}^T \\gamma_t(i,j)  } { \\sum_{j=1}^{|S|} \\sum_{t=1}^T \\gamma_t(i,j) } $   \n",
    "= {expected number of transitions from $s_i$ to $s_j$ } / {expected number of transitions out of $s_i$ }  \n",
    "$B_{ij} = \\frac { \\sum_{i=1}^{|S|} \\sum_{t=1}^T [x_y=v_k] \\gamma_t(i,j)  } { \\sum_{i=1}^{|S|} \\sum_{t=1}^T \\gamma_t(i,j) }$   \n",
    "= { expected number of observations $v_k$ occurs in state $s_i$ } / { expected number of times in state $s_i$ }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "71d66890-17d3-4776-abf2-cfe3fa966a4a"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
